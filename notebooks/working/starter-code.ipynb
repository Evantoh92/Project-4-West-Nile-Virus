{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4 - WEST NILE VIRUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "West Nile virus can cause a fatal neurological disease in humans. It is mainly transmitted to people through the bites of infected mosquitoes. Treatments often involve hospitalization, intravenous fluids, respiratory support, and prevention of secondary infections. \n",
    "\n",
    "No vaccine is available for humans. \n",
    "\n",
    "Studies estimated the economic impact of the WNV disease outbreak in 2002 in Louisiana, which resulted in 24 deaths. Total epidemic costs were about 20.14 million for the 329 cases, including 9.2 million for mosquito control and public health agency costs. As mosquito control is an expensive exercise, the state would like our team to propose a cost-effective plan for pesticide deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "\n",
    "If you want to, it's great to use relative links to direct your audience to various sections of a notebook. **HERE'S A DEMONSTRATION WITH THE CURRENT SECTION HEADERS**:\n",
    "\n",
    "### Contents: (TOBE UPDATED)\n",
    "- [2017 Data Import & Cleaning](#Data-Import-and-Cleaning)\n",
    "- [2018 Data Import and Cleaning](#2018-Data-Import-and-Cleaning)\n",
    "- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "- [Data Visualization](#Visualize-the-data)\n",
    "- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n",
    "- [Outside Research](#Outside-Research)\n",
    "- [Conclusions and Recommendations](#Conclusions-and-Recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you combine your problem statement, executive summary, data dictionary, and conclusions/recommendations, you have an amazing README.md file that quickly aligns your audience to the contents of your project.** Don't forget to cite your data sources!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*All libraries used should be added here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.image import imread\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "weather = pd.read_csv('../data/weather.csv')\n",
    "spray = pd.read_csv('../data/spray.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN & TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete variables that are not useful\n",
    "# address related are deleted as location is determined by Latitude and Longuitude\n",
    "# checked online all species are relevant for WNV\n",
    "def del_var(data):\n",
    "    try:\n",
    "        del data['Address']\n",
    "        del data['Block']\n",
    "        del data['Street']\n",
    "        del data['AddressNumberAndStreet']\n",
    "        del data['AddressAccuracy']\n",
    "        del data['Species']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_var(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_var(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine test results for traps with multiple rows in any single day, since we are ignoring species\n",
    "import datetime\n",
    "def aggregate_train(data):\n",
    "    grouped = data.groupby(['Date', 'Trap', 'Latitude', 'Longitude'])\n",
    "    aggregated = pd.DataFrame(grouped.agg({'NumMosquitos': np.sum, 'WnvPresent': np.max})).reset_index()\n",
    "    aggregated.sort_values(by='NumMosquitos', ascending = False)\n",
    "    aggregated.sort_values(by='Date', inplace=True)\n",
    "    aggregated = aggregated.reset_index(drop = True)\n",
    "    aggregated['Date'] = pd.to_datetime(aggregated.Date, format='%Y-%m-%d')\n",
    "    aggregated['Year'] = pd.DatetimeIndex(aggregated['Date']).year \n",
    "    aggregated['Month'] = pd.DatetimeIndex(aggregated['Date']).month\n",
    "    return aggregated\n",
    "\n",
    "def aggregate_test(data):\n",
    "    data['Date'] = pd.to_datetime(data.Date, format='%Y-%m-%d')\n",
    "    data['Year'] = pd.DatetimeIndex(data['Date']).year \n",
    "    data['Month'] = pd.DatetimeIndex(data['Date']).month\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train = aggregate_train(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_test = aggregate_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4616 entries, 0 to 4615\n",
      "Data columns (total 8 columns):\n",
      "Date            4616 non-null datetime64[ns]\n",
      "Trap            4616 non-null object\n",
      "Latitude        4616 non-null float64\n",
      "Longitude       4616 non-null float64\n",
      "NumMosquitos    4616 non-null int64\n",
      "WnvPresent      4616 non-null int64\n",
      "Year            4616 non-null int64\n",
      "Month           4616 non-null int64\n",
      "dtypes: datetime64[ns](1), float64(2), int64(4), object(1)\n",
      "memory usage: 288.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# there are no missing data in agg_train\n",
    "agg_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007    1459\n",
       "2013    1163\n",
       "2009    1006\n",
       "2011     988\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_train.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are no missing data in agg_test\n",
    "agg_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2010    36557\n",
       "2008    30498\n",
       "2012    27115\n",
       "2014    22123\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_test.Year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# source: https://www.johndcook.com/blog/python_longitude_latitude/\n",
    "def distance_on_unit_sphere(lat1, long1, lat2, long2):\n",
    "\n",
    "    # Convert latitude and longitude to spherical coordinates in radians\n",
    "    degrees_to_radians = math.pi/180.0\n",
    "\n",
    "    # phi = 90 - latitude\n",
    "    phi1 = (90.0 - lat1)*degrees_to_radians\n",
    "    phi2 = (90.0 - lat2)*degrees_to_radians\n",
    "\n",
    "    # theta = longitude\n",
    "    theta1 = long1*degrees_to_radians\n",
    "    theta2 = long2*degrees_to_radians\n",
    "\n",
    "    # Compute spherical distance from spherical coordinates.\n",
    "\n",
    "    # For two locations in spherical coordinates\n",
    "    # (1, theta, phi) and (1, theta', phi')\n",
    "    # cosine( arc length ) =\n",
    "    # sin phi sin phi' cos(theta-theta') + cos phi cos phi'\n",
    "    # distance = rho * arc length\n",
    "\n",
    "    cos = (math.sin(phi1)*math.sin(phi2)*math.cos(theta1 - theta2) +\n",
    "    math.cos(phi1)*math.cos(phi2))\n",
    "    arc = math.acos( cos )\n",
    "\n",
    "    # Remember to multiply arc by the radius of the earth\n",
    "    # in your favorite set of units to get length.\n",
    "    return arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather stations 1 and 2 lat/longitude\n",
    "# Station 1: CHICAGO O'HARE INTERNATIONAL AIRPORT Lat: 41.995 Lon: -87.933 Elev: 662 ft. above sea level\n",
    "# Station 2: CHICAGO MIDWAY INTL ARPT Lat: 41.786 Lon: -87.752 Elev: 612 ft. above sea level\n",
    "station_1_lat = 41.995\n",
    "station_1_lon = -87.933\n",
    "station_2_lat = 41.786\n",
    "station_2_lon = -87.752\n",
    "# function to calculate whether station 1 or 2 (from weather.csv) is closer\n",
    "def closest_station(lat, lon):\n",
    "    if (distance_on_unit_sphere(lat, lon, station_1_lat, station_1_lon) <\n",
    "        distance_on_unit_sphere(lat, lon, station_2_lat, station_2_lon)):\n",
    "        return 1\n",
    "    else: return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REVISED 3/8/2019\n",
    "# add station to indicate whether station 1 or 2 is closer\n",
    "# create unique variable to combine train, weather and spray data\n",
    "def station_var (data):\n",
    "    data['Station'] = [closest_station(a,b) for (a, b) in zip(data.Latitude, data.Longitude)]\n",
    "    data['unique'] =  data['Station'].astype(str) + str(\": \") + data['Date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_var(agg_train)\n",
    "station_var(agg_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  WEATHER DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station          int64\n",
       "Date            object\n",
       "Tmax             int64\n",
       "Tmin             int64\n",
       "Tavg            object\n",
       "Depart          object\n",
       "DewPoint         int64\n",
       "WetBulb         object\n",
       "Heat            object\n",
       "Cool            object\n",
       "Sunrise         object\n",
       "Sunset          object\n",
       "CodeSum         object\n",
       "Depth           object\n",
       "Water1          object\n",
       "SnowFall        object\n",
       "PrecipTotal     object\n",
       "StnPressure     object\n",
       "SeaLevel        object\n",
       "ResultSpeed    float64\n",
       "ResultDir        int64\n",
       "AvgSpeed        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Many numerical columns are imported as objects\n",
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date to date format\n",
    "weather['Date'] = pd.to_datetime(weather['Date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "(\"'list' object is not callable\", 'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-219fc12250b1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Calculate missing Tavg as average of Tmax and Tmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mweather\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tavg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweather\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTmax\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTmin\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTavg\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'M'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTavg\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mweather\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tavg'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweather\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Tavg'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6012\u001b[0m                          \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6013\u001b[0m                          kwds=kwds)\n\u001b[1;32m-> 6014\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6016\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    140\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;31m# compute the result using the series generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 277\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    278\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: (\"'list' object is not callable\", 'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "# Calculate missing Tavg as average of Tmax and Tmin\n",
    "weather['Tavg'] = weather.apply([lambda row: (row.Tmax + row.Tmin) / 2 if row.Tavg == 'M' else row.Tavg], axis=1)\n",
    "weather['Tavg'] = pd.to_numeric(weather['Tavg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing WetBulb from the other station\n",
    "for k, v in enumerate(weather['WetBulb']):\n",
    "    if v == 'M' and weather.iloc[k]['Station'] == 1:\n",
    "        weather.loc[k,'WetBulb'] = weather['WetBulb'][k+1]\n",
    "    if v == 'M' and weather.iloc[k]['Station'] == 2:\n",
    "        weather.loc[k,'WetBulb'] = weather['WetBulb'][k-1]\n",
    "weather['WetBulb'] = pd.to_numeric(weather['WetBulb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineer feature for daytime in minutes\n",
    "sunset = pd.to_timedelta([i[:2] + ':' + i[2:] + ':00' for i in weather[weather['Station']==1]['Sunset']])\n",
    "sunrise = pd.to_timedelta([i[:2] + ':' + i[2:] + ':00' for i in weather[weather['Station']==1]['Sunrise']])\n",
    "daytime = sunset - sunrise\n",
    "daytime_m = list([int(str(i).split(\":\")[0][-2:]) * 60 + int(str(i).split(\":\")[1]) for i in daytime])\n",
    "weather['Daytime'] = list(sum((zip(daytime_m,daytime_m)),()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill trace values as 0.005\n",
    "weather['PrecipTotal'].replace('  T', '0.005', inplace=True)\n",
    "# Fill missing values from the other Station\n",
    "for k, v in enumerate(weather['PrecipTotal']):\n",
    "    if v == 'M' and weather.iloc[k]['Station'] == 1:\n",
    "        weather.loc[k,'PrecipTotal'] = weather['PrecipTotal'][k+1]\n",
    "    if v == 'M' and weather.iloc[k]['Station'] == 2:\n",
    "        weather.loc[k,'PrecipTotal'] = weather['PrecipTotal'][k-1]\n",
    "weather['PrecipTotal'] = pd.to_numeric(weather['PrecipTotal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values for Depart, Heat, Cool, AvgSpeed from the other station\n",
    "cols = ['Depart', 'Heat', 'Cool', 'AvgSpeed']\n",
    "for col in cols:\n",
    "    for k, v in enumerate(weather[col]):\n",
    "        if v == 'M' and weather.iloc[k]['Station'] == 1:\n",
    "            weather.loc[k,col] = weather[col][k+1]\n",
    "        if v == 'M' and weather.iloc[k]['Station'] == 2:\n",
    "            weather.loc[k,col] = weather[col][k-1]\n",
    "    weather[col] = pd.to_numeric(weather[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Station                 int64\n",
       "Date           datetime64[ns]\n",
       "Tmax                    int64\n",
       "Tmin                    int64\n",
       "Tavg                   object\n",
       "Depart                  int64\n",
       "DewPoint                int64\n",
       "WetBulb                object\n",
       "Heat                    int64\n",
       "Cool                    int64\n",
       "Sunrise                object\n",
       "Sunset                 object\n",
       "CodeSum                object\n",
       "Depth                  object\n",
       "Water1                 object\n",
       "SnowFall               object\n",
       "PrecipTotal           float64\n",
       "StnPressure            object\n",
       "SeaLevel               object\n",
       "ResultSpeed           float64\n",
       "ResultDir               int64\n",
       "AvgSpeed              float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column data types after cleaning\n",
    "# We skipped cleaning for CodeSum, Depth, Water1, SnowFall, StnPressure, SeaLevel\n",
    "# because we do not find them useful\n",
    "weather.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine weather & train/test data for ease of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unique variable to combine train and weather data\n",
    "weather['unique'] = weather['Station'].astype(str) + str(\": \") + weather['Date'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train = pd.merge(agg_train, weather, how=\"inner\", on=\"unique\")\n",
    "combined_test = pd.merge(agg_test, weather, how=\"inner\", on=\"unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping duplicate and unnecessary variables\n",
    "combined_train = combined_train.drop([\"Date_y\", \"Station_y\", \"unique\"], axis=1)\n",
    "combined_train.rename(columns={\"Date_x\": \"Date\"}, inplace=True)\n",
    "combined_train.rename(columns={\"Station_x\": \"Station\"}, inplace=True)\n",
    "combined_test = combined_test.drop([\"Date_y\", \"Station_y\", \"unique\"], axis=1)\n",
    "combined_test.rename(columns={\"Station_x\": \"Station\"}, inplace=True)\n",
    "\n",
    "# NEW 3/8/2019 - to create YYMM\n",
    "# bin the surveillance efforts by month\n",
    "combined_train [\"YYMM\"] = tuple(zip(combined_train.Year, combined_train.Month))\n",
    "combined_test [\"YYMM\"] = tuple(zip(combined_test.Year, combined_test.Month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4616 entries, 0 to 4615\n",
      "Data columns (total 30 columns):\n",
      "Date            4616 non-null datetime64[ns]\n",
      "Trap            4616 non-null object\n",
      "Latitude        4616 non-null float64\n",
      "Longitude       4616 non-null float64\n",
      "NumMosquitos    4616 non-null int64\n",
      "WnvPresent      4616 non-null int64\n",
      "Year            4616 non-null int64\n",
      "Month           4616 non-null int64\n",
      "Station         4616 non-null int64\n",
      "Tmax            4616 non-null int64\n",
      "Tmin            4616 non-null int64\n",
      "Tavg            4616 non-null object\n",
      "Depart          4616 non-null int64\n",
      "DewPoint        4616 non-null int64\n",
      "WetBulb         4616 non-null object\n",
      "Heat            4616 non-null int64\n",
      "Cool            4616 non-null int64\n",
      "Sunrise         4616 non-null object\n",
      "Sunset          4616 non-null object\n",
      "CodeSum         4616 non-null object\n",
      "Depth           4616 non-null object\n",
      "Water1          4616 non-null object\n",
      "SnowFall        4616 non-null object\n",
      "PrecipTotal     4616 non-null float64\n",
      "StnPressure     4616 non-null object\n",
      "SeaLevel        4616 non-null object\n",
      "ResultSpeed     4616 non-null float64\n",
      "ResultDir       4616 non-null int64\n",
      "AvgSpeed        4616 non-null float64\n",
      "YYMM            4616 non-null object\n",
      "dtypes: datetime64[ns](1), float64(5), int64(12), object(12)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "combined_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2007, 5)      18\n",
       "(2007, 6)     120\n",
       "(2007, 7)     260\n",
       "(2007, 8)     574\n",
       "(2007, 9)     382\n",
       "(2007, 10)    105\n",
       "(2009, 5)      41\n",
       "(2009, 6)     255\n",
       "(2009, 7)     285\n",
       "(2009, 8)     180\n",
       "(2009, 9)     208\n",
       "(2009, 10)     37\n",
       "(2011, 6)     193\n",
       "(2011, 7)     263\n",
       "(2011, 8)     252\n",
       "(2011, 9)     280\n",
       "(2013, 6)     248\n",
       "(2013, 7)     285\n",
       "(2013, 8)     357\n",
       "(2013, 9)     273\n",
       "Name: YYMM, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many trap observations per month\n",
    "combined_train[\"YYMM\"].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2007    1459\n",
       "2013    1163\n",
       "2009    1006\n",
       "2011     988\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_train.Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning - Spray Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEW 3/8/2019 - to create YYMM\n",
    "spray['Date'] = pd.to_datetime(spray.Date, format='%Y-%m-%d')\n",
    "spray['Year'] = pd.DatetimeIndex(spray['Date']).year \n",
    "spray['Month'] = pd.DatetimeIndex(spray['Date']).month\n",
    "spray[\"YYMM\"] = tuple(zip(spray.Year, spray.Month))\n",
    "del spray['Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14835 entries, 0 to 14834\n",
      "Data columns (total 6 columns):\n",
      "Date         14835 non-null datetime64[ns]\n",
      "Latitude     14835 non-null float64\n",
      "Longitude    14835 non-null float64\n",
      "Year         14835 non-null int64\n",
      "Month        14835 non-null int64\n",
      "YYMM         14835 non-null object\n",
      "dtypes: datetime64[ns](1), float64(2), int64(2), object(1)\n",
      "memory usage: 695.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# there are no missing data in spray\n",
    "spray.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013    12626\n",
       "2011     2209\n",
       "Name: Year, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spray.Year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps of WNV by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source: https://www.kaggle.com/neilsummers/west-nile-heatmap-by-year\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "mapdata = plt.imread('../data/map1.png')\n",
    "traps = pd.read_csv('../data/train.csv', parse_dates=['Date'])[['Date', 'Trap', 'Longitude', 'Latitude', 'WnvPresent']]\n",
    "\n",
    "alpha_cm = plt.cm.Reds\n",
    "alpha_cm._init()\n",
    "alpha_cm._lut[:-3,-1] = np.linspace(0, 1, alpha_cm.N)\n",
    "aspect = mapdata.shape[0] / mapdata.shape[1]\n",
    "lon_lat_box = (-88, -87.5, 41.6, 42.1)\n",
    "\n",
    "plt.figure(figsize=(18,24))\n",
    "for year, subplot in zip([2007, 2009, 2011, 2013], [221, 222, 223, 224]):\n",
    "    sightings = traps[(traps['WnvPresent'] > 0) & (traps['Date'].apply(lambda x: x.year) == year)]\n",
    "    sightings = sightings.groupby(['Date', 'Trap', 'Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "    X = sightings[['Longitude', 'Latitude']].values\n",
    "    kd = KernelDensity(bandwidth=0.02)\n",
    "    kd.fit(X)\n",
    "\n",
    "    xv,yv = np.meshgrid(np.linspace(-88, -87.5, 100), np.linspace(41.6, 42.1, 100))\n",
    "    gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "    zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "    plt.subplot(subplot)\n",
    "    plt.gca().set_title(year)\n",
    "    plt.imshow(mapdata,\n",
    "               extent=lon_lat_box,\n",
    "               aspect=aspect)\n",
    "    plt.imshow(zv, \n",
    "               origin='lower',\n",
    "               cmap=alpha_cm,\n",
    "               extent=lon_lat_box,\n",
    "               aspect=aspect)\n",
    "    plt.tight_layout()\n",
    "    locations = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "    plt.scatter(locations[:,0], locations[:,1], c='blue', marker='x')\n",
    "    plt.scatter([station_1_lon, station_2_lon], [station_1_lat,station_2_lat], c='purple', marker='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps of Spray activities by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapdata = imread('../data/map2.png')\n",
    "\n",
    "lon_lat_box = (-88.15, -87.5, 41.6, 42.45)\n",
    "\n",
    "plt.figure(figsize=(18,14))\n",
    "for year, subplot in zip([2011, 2013], [121, 122]):\n",
    "    sightings = traps[(traps['WnvPresent'] > 0) & (traps['Date'].apply(lambda x: x.year) == year)]\n",
    "    sightings = sightings.groupby(['Date', 'Trap','Longitude', 'Latitude']).max()['WnvPresent'].reset_index()\n",
    "    X = sightings[['Longitude', 'Latitude']].values\n",
    "    kd = KernelDensity(bandwidth=0.02)\n",
    "    kd.fit(X)\n",
    "\n",
    "    xv,yv = np.meshgrid(np.linspace(-88.15, -87.5, 100), np.linspace(41.6, 42.45, 100))\n",
    "    gridpoints = np.array([xv.ravel(),yv.ravel()]).T\n",
    "    zv = np.exp(kd.score_samples(gridpoints).reshape(100,100))\n",
    "    plt.subplot(subplot)\n",
    "    plt.gca().set_title(year)\n",
    "    plt.imshow(mapdata,\n",
    "               extent=lon_lat_box,\n",
    "               aspect=aspect)\n",
    "    plt.imshow(zv, \n",
    "               origin='lower',\n",
    "               cmap=alpha_cm,\n",
    "               extent=lon_lat_box,\n",
    "               aspect=aspect)\n",
    "    plt.tight_layout()\n",
    "    locations_trap = traps[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "    plt.scatter(locations_trap[:,0], locations_trap[:,1], c='blue', marker='x')\n",
    "    plt.scatter([station_1_lon, station_2_lon], [station_1_lat,station_2_lat], c='purple', marker='o')\n",
    "    locations_spray = spray[['Longitude', 'Latitude']].drop_duplicates().values\n",
    "    plt.scatter(locations_spray[:,0], locations_spray[:,1], c='orange', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairplot for weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at relationships between features describing temperature\n",
    "sns.pairplot(weather[['Tmax', 'Tmin', 'Tavg', 'DewPoint', 'WetBulb']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the trap locations in comparison with the two weather stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not necessary anymore\n",
    "\"\"\"\n",
    "# look at dispersion of traps geographically\n",
    "plt.scatter(agg_train.Longitude, agg_train.Latitude, c = agg_train.Station)\n",
    "plt.scatter([station_1_lon, station_2_lon], [station_1_lat,station_2_lat], c = 'r')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 20 locations of highest number of mosquitoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## revised 5/8/2019\n",
    "#top 20 locations of highest number of mosquitoes\n",
    "lat_long = combined_train.groupby([\"Latitude\",\"Longitude\"])['NumMosquitos'].agg(['mean','count'])\n",
    "lat_long= lat_long.sort_values(by=\"mean\", ascending=False).head(20)\n",
    "#total_mosquitoes = total_mosquitoes.reset_index()\n",
    "#sum = total no. of mosquitoes in the location\n",
    "#count = no. of counts (by unique days)\n",
    "\n",
    "lat_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## revised 5/8/2019\n",
    "#period with highest number of mosquitoes \n",
    "total_mosquitoes = combined_train.groupby([\"YYMM\"])['NumMosquitos'].agg(['mean','count'])\n",
    "total_mosquitoes= total_mosquitoes.sort_values(by=\"mean\", ascending=False).head(20)\n",
    "#total_mosquitoes = total_mosquitoes.reset_index()\n",
    "\n",
    "total_mosquitoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## revised 5/8/2019\n",
    "#period with highest number of WNV presents\n",
    "total_WNV = combined_train.groupby([\"YYMM\"])['WnvPresent'].agg(['mean','count'])\n",
    "total_WNV= total_WNV.sort_values(by=\"mean\", ascending=False).head(20)\n",
    "\n",
    "total_WNV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## revised 5/8/2019\n",
    "#traps corresponding to the locations of highest number of mosquitoes \n",
    "total_mosquitoes = combined_train.groupby([\"Trap\"])['NumMosquitos'].agg(['mean','count'])\n",
    "total_mosquitoes= total_mosquitoes.sort_values(by=\"mean\", ascending=False).head(20)\n",
    "#total_mosquitoes = total_mosquitoes.reset_index()\n",
    "\n",
    "total_mosquitoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No. of Mosquitoes by Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_train_ = combined_train[combined_train[\"WnvPresent\"]== 1]\n",
    "combined_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = combined_train_.groupby([\"Tavg\",\"WetBulb\",\"Year\"])['NumMosquitos'].agg(['sum']).reset_index()\n",
    "temp = pd.DataFrame(temp)\n",
    "temp.sort_values(\"Tavg\")\n",
    "temp.to_csv(r'../data/temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.bar(temp['Tavg'],temp['sum'])\n",
    "plt.title(\"Total number of mosquitoes with virus at different average temperatures\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"No. of mosquitoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traps corresponding to the locations of highest number of mosquitoes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#traps corresponding to the locations of highest number of mosquitoes \n",
    "total_mosquitoes = combined_train_.groupby([\"Trap\"])['NumMosquitos'].agg(['sum','count'])\n",
    "total_mosquitoes= total_mosquitoes.sort_values(by=\"sum\", ascending=False).head(20).reset_index()\n",
    "#total_mosquitoes = total_mosquitoes.reset_index()\n",
    "\n",
    "total_mosquitoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "plt.bar(total_mosquitoes[\"Trap\"],total_mosquitoes[\"sum\"])\n",
    "plt.title(\"Total number of mosquitoes in traps with virus\")\n",
    "plt.xlabel(\"Traps with virus\")\n",
    "plt.ylabel(\"No. of mosquitoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grouping data into years as spray data contains only 2011 and 2013 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_var={}\n",
    "def grouping_by_year(data):\n",
    "    grps = data.groupby(\"Year\")\n",
    "    grps_key = list(grps.groups.keys())\n",
    "    for i in range(len(list(grps.groups.keys()))):\n",
    "        list_var[\"grouped_data_{}\".format(grps_key[i])] = grps.get_group(grps_key[i])\n",
    "    return list_var, grps_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_by_year(combined_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_2007 = list_var[\"grouped_data_2007\"]\n",
    "grouped_data_2009 = list_var[\"grouped_data_2009\"]\n",
    "grouped_data_2011 = list_var[\"grouped_data_2011\"]\n",
    "grouped_data_2013 = list_var[\"grouped_data_2013\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_var2={}\n",
    "def grouping_by_year_spray(data):\n",
    "    grps = data.groupby(\"Year\")\n",
    "    grps_key = list(grps.groups.keys())\n",
    "    for i in range(len(list(grps.groups.keys()))):\n",
    "        list_var2[\"grouped_data_spray{}\".format(grps_key[i])] = grps.get_group(grps_key[i])\n",
    "    return list_var2, grps_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouping_by_year_spray(spray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_spray_2011 = list_var2[\"grouped_data_spray2011\"]\n",
    "grouped_data_spray_2013 = list_var2[\"grouped_data_spray2013\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the trap locations with mosquitoes in comparison with the spray locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 2011\n",
    "plt.scatter(grouped_data_spray_2011.Longitude, grouped_data_spray_2011.Latitude, c = 'r')\n",
    "plt.scatter(grouped_data_2011.Longitude, grouped_data_2011.Latitude, c = grouped_data_2011.NumMosquitos)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# 2013\n",
    "plt.scatter(grouped_data_spray_2013.Longitude, grouped_data_spray_2013.Latitude, c = 'r')\n",
    "plt.scatter(grouped_data_2013.Longitude, grouped_data_2013.Latitude, c = grouped_data_2013.NumMosquitos)\n",
    "plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nummos_bydate = combined_train.groupby([\"Date\"])['NumMosquitos'].agg(['sum']).reset_index()\n",
    "Nummos_bydate = pd.DataFrame(Nummos_bydate )\n",
    "Nummos_bydate .sort_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spray[\"spray\"] = 1\n",
    "Spray_bydate = spray.groupby([\"Date\"])[\"spray\"].agg(['sum']).reset_index()\n",
    "Spray_bydate = pd.DataFrame(Spray_bydate )\n",
    "Spray_bydate .sort_values(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_spray_train = pd.merge(Spray_bydate, Nummos_bydate, how=\"outer\", left_on=\"Date\", right_on=\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean the data frame for combined spray/train data\n",
    "combined_spray_train.Date.fillna(combined_spray_train.Date, inplace=True)\n",
    "del combined_spray_train['Date']\n",
    "combined_spray_train.rename(columns={\"sum_x\": \"Num_spray\", \"sum_y\": \"NumMosquitos\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_spray_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dispersion of virus incidence geographically\n",
    "plt.scatter(agg_train.Longitude, agg_train.Latitude, c = agg_train.WnvPresent, alpha = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at incidence of cases over months\n",
    "agg_train[['Month', 'NumMosquitos']].groupby(\"Month\").sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at incidence of cases over months\n",
    "agg_train[['Month', 'WnvPresent']].groupby(\"Month\").sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at incidence of cases over years\n",
    "agg_train[['Year', 'WnvPresent']].groupby('Year').sum().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm, linear_model, datasets\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_train.drop([\"Latitude\", \"Longitude\", \"WnvPresent\", \"Trap\", \"Date\", \"YYMM\", \"Sunrise\", \"Sunset\", \"CodeSum\", \"Depth\", \"Water1\", \"SnowFall\", \"StnPressure\", \"SeaLevel\"], axis=1)\n",
    "y = combined_train.WnvPresent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_evaluation:\n",
    "    \n",
    "    def __init__(self, y_test, predicted_value):\n",
    "        self.y_test = y_test\n",
    "        self.predicted_value = predicted_value\n",
    "        \n",
    "    def confusion_matrix (self):\n",
    "        tn, fp, fn, tp = confusion_matrix(self.y_test, self.predicted_value).ravel()\n",
    "        print(\"True Negatives: %s\" % tn)\n",
    "        print(\"False Positives: %s\" % fp)\n",
    "        print(\"False Negatives: %s\" % fn)\n",
    "        print(\"True Positives: %s\" % tp)\n",
    "        precision = tp/(tp+fp)\n",
    "        recall = tp/(tp+fn)\n",
    "        print(\"Precision: {}\" .format (tp/(tp+fp)))\n",
    "        print(\"Recall: {}\" .format (tp/(tp+fn)))\n",
    "        F1_score = 2*((precision*recall)/(precision+recall))\n",
    "        print (\"F1 score: {}\" .format(2*((precision*recall)/(precision+recall))))\n",
    "        self.score = F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predicted_lr = lr.predict(X_test)\n",
    "lrg = model_evaluation (y_test,predicted_lr)\n",
    "lrg.confusion_matrix ()\n",
    "lr.score (X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test code, scaling of data, reduction of dimensionality, use of SVC.\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "pipeline = Pipeline([\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', SVC(kernel = 'linear', C = 1))])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__C=np.logspace(-4, 1, 6),\n",
    "                  clf__kernel=['rbf','linear'])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=5, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "print(grid.score(X_test, y_test))\n",
    "\n",
    "\n",
    "''' sample codes\n",
    "#params={'model__C':[.01,.05,.1,.5,1,5,10],\n",
    "           'model__penalty':['l1','l2']}\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df=40,ngram_range=(1,4))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('model',LogisticRegression())])\n",
    "grid = GridSearchCV(pipeline, cv=5, n_jobs=-1, param_grid=params ,scoring='roc_auc')\n",
    "grid.fit(train['text'], train['output'])\n",
    "grid.score(test['text'], test['output'])\n",
    "\n",
    "\n",
    "#pipe = Pipeline([\n",
    "        ('scale', StandardScaler()),\n",
    "        ('reduce_dims', PCA(n_components=4)),\n",
    "        ('clf', SVC(kernel = 'linear', C = 1))])\n",
    "\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__C=np.logspace(-4, 1, 6),\n",
    "                  clf__kernel=['rbf','linear'])\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=param_grid, cv=3, n_jobs=1, verbose=2, scoring= 'accuracy')\n",
    "grid.fit(X, y)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', SVC(kernel = 'linear', C = 1))])\n",
    "param_grid = dict(clf__C=np.logspace(-4, 1, 6),\n",
    "                  clf__kernel=['rbf','linear'])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', KNeighborsClassifier())])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__n_neighbors=range(1, 201, 10),\n",
    "                  clf__metric=['euclidean', 'manhattan'])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', KNeighborsClassifier())])\n",
    "param_grid = dict(clf__n_neighbors=range(1, 201, 10),\n",
    "                  clf__metric=['euclidean', 'manhattan'])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multinomial is not applicable here due to the need for vectorizing, Decision trees will be used\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', DecisionTreeClassifier())])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__max_depth=[None,1,2,3,4,5],\n",
    "                  clf__max_features=['sqrt', 'log2', None],\n",
    "                  clf__min_samples_split=[2,3,4,5,10,15,20,25,30,40,50])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)\n",
    "\n",
    "\n",
    "'''\n",
    "n_params = {\n",
    "    'max_depth': [None,1,2,3,4,5],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'min_samples_split': [2,3,4,5,10,15,20,25,30,40,50]\n",
    "    }\n",
    "\n",
    "grid3 = GridSearchCV(DecisionTreeClassifier(), n_params, cv=5)\n",
    "grid3.fit(X,y)\n",
    "print ('best score = {}'.format(grid3.best_score_))\n",
    "grid3.best_params_\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[2][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', DecisionTreeClassifier())])\n",
    "param_grid = dict(clf__max_depth=[None,1,2,3,4,5],\n",
    "                  clf__max_features=['sqrt', 'log2', None],\n",
    "                  clf__min_samples_split=[2,3,4,5,10,15,20,25,30,40,50])\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=5, n_jobs=-1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "print(grid.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of feature importance: grid.best_estimator_.steps[1][1].feature_importances_\n",
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[1][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random forest- of course we need a random search first yeah?\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "'''\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "'''\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "#rc = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#rc_random = RandomizedSearchCV(estimator = rc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', RandomForestClassifier(random_state=42))])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__n_estimators=[int(x) for x in np.linspace(200, 2000, 10)],\n",
    "                  clf__max_features=['auto', 'sqrt'],\n",
    "                  clf__max_depth=max_depth,\n",
    "                  clf__min_samples_split=[2, 5, 10],\n",
    "                  clf__min_samples_leaf=[1, 2, 4],\n",
    "                  clf__bootstrap=[True, False]\n",
    "                 )\n",
    "\n",
    "\n",
    "rc_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "rc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=8)),\n",
    "    ('clf', RandomForestClassifier(max_features='auto', bootstrap=True, min_samples_leaf=2,random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__n_estimators=[700, 800, 900],\n",
    "                  clf__max_depth=[7,9,11,13],\n",
    "                  clf__min_samples_split=[8,10,12,14],\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[2][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(200, 2000, 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, 10)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "'''\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "'''\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "#rc = RandomForestClassifier()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "#rc_random = RandomizedSearchCV(estimator = rc, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(random_state=42))])\n",
    "param_grid = dict(clf__n_estimators=[int(x) for x in np.linspace(200, 2000, 10)],\n",
    "                  clf__max_features=['auto', 'sqrt'],\n",
    "                  clf__max_depth=max_depth,\n",
    "                  clf__min_samples_split=[2, 5, 10],\n",
    "                  clf__min_samples_leaf=[1, 2, 4],\n",
    "                  clf__bootstrap=[True, False]\n",
    "                 )\n",
    "\n",
    "\n",
    "rc_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "rc_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rc_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', RandomForestClassifier(max_features='sqrt', bootstrap=True,random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__n_estimators=[1300, 1400, 1500],\n",
    "                  clf__max_depth=[7,9,11,13],\n",
    "                  clf__min_samples_split=[4,5,6],\n",
    "                  clf__min_samples_leaf=[4,6,8]\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[1][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', AdaBoostClassifier(random_state=42))])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__n_estimators=[150, 200],\n",
    "                  clf__learning_rate=[0.01,0.05,0.1,0.3,1],\n",
    "                 )\n",
    "\n",
    "\n",
    "ad_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "ad_random.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=6)),\n",
    "    ('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__n_estimators=[200,250],\n",
    "                  clf__learning_rate=[0.1,0.2,0.25,0.3,0.35]\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[2][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaboost without PCA\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', AdaBoostClassifier(random_state=42))])\n",
    "param_grid = dict(clf__n_estimators=[150, 200],\n",
    "                  clf__learning_rate=[0.01,0.05,0.1,0.3,1],\n",
    "                 )\n",
    "\n",
    "\n",
    "ad_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "ad_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', AdaBoostClassifier(random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__n_estimators=[200,250],\n",
    "                  clf__learning_rate=[0.1,0.2,0.25,0.3,0.35]\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[1][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradientboosting with PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=4)),\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))])\n",
    "param_grid = dict(reduce_dims__n_components=[4,6,8],\n",
    "                  clf__min_samples_split=[50, 100],\n",
    "                  clf__min_samples_leaf=[1,2,4],\n",
    "                  clf__max_features=['auto','sqrt', 'log2'],\n",
    "                  clf__max_depth = [5,6,7,8]\n",
    "                 )\n",
    "\n",
    "gb_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('reduce_dims', PCA(n_components=8)),\n",
    "    ('clf', GradientBoostingClassifier(min_samples_leaf=2, max_depth= 6,random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__min_samples_split=[100,150],\n",
    "                  clf__max_features=['auto','sqrt', 'log2']\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[2][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradientboosting without PCA\n",
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', GradientBoostingClassifier(random_state=42))])\n",
    "param_grid = dict(clf__min_samples_split=[50, 100],\n",
    "                  clf__min_samples_leaf=[1,2,4],\n",
    "                  clf__max_features=['auto','sqrt', 'log2'],\n",
    "                  clf__max_depth = [5,6,7,8]\n",
    "                 )\n",
    "\n",
    "gb_random = RandomizedSearchCV(pipeline, cv=3, n_jobs=-1, param_distributions=param_grid, n_iter = 100, verbose=2, scoring='roc_auc', random_state=42)\n",
    "gb_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    #('poly_feat', PolynomialFeatures(degree=3, interaction_only=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "    ('clf', GradientBoostingClassifier(min_samples_leaf=1, max_features= 'sqrt',random_state=42))])\n",
    "\n",
    "param_grid = dict(clf__min_samples_split=[30,40,50],\n",
    "                  clf__max_depth=[3,4,5]\n",
    "                 )\n",
    "\n",
    "\n",
    "grid = GridSearchCV(pipeline, cv=3, n_jobs=1, param_grid=param_grid, verbose=2, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_score_)\n",
    "print(grid.cv_results_)\n",
    "grid.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_2 = zip(list(X.columns), list(grid.best_estimator_.steps[1][1].feature_importances_))\n",
    "pl = pd.Series(dict(list_2)).sort_values(ascending=False)\n",
    "pl.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some takeaways, random forest seems to do the best. PCA did not really help much and could be due to small number of features. Adaboost learning rate seems to balance off with the estimators. higher estimators = lower learning rate. When doing decision trees, features that are the most important is heavily skewed to number of mosquitos. Baseline should be treated as 94% because this is supposed to be rare. Considering a binary classification problem, one can always assume the mosquito is not infected and still get it right 94% of the time.\n",
    "\n",
    "Moving forward: Makes sense to predict the number of mosquitos first? then do a classification? Read other solutions. Find ways to process the features, date/time stuff and encoding of some of the categorical ones. Break the baseline.\n",
    "\n",
    "Done: Pipelines Gridsearch Randomized search PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
